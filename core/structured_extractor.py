"""
Structured Process Tracing Extractor
Modern approach using LiteLLM + Pydantic schemas instead of prompt engineering
Replaces manual JSON parsing with schema-enforced extraction
"""

import os
import json
from datetime import datetime
from typing import Optional, Dict, Any
from dotenv import load_dotenv

# Load environment
load_dotenv()

# Import our schemas and UniversalLLM
from core.structured_schema import (
    ProcessTracingGraph, 
    ProcessTracingNode, 
    ProcessTracingEdge,
    StructuredExtractionResult,
    ExtractionMetadata
)
import litellm

# Van Evera Process Tracing Ontology-Aware Extraction Prompt
STRUCTURED_EXTRACTION_PROMPT = """Extract causal relationships from this text using Van Evera process tracing methodology with the following ontology:

## NODE TYPES (8 types):

**Event**: Specific occurrences in time/space
- Properties: description (required), timestamp, location, certainty (0.0-1.0), type (triggering/intermediate/outcome/unspecified), date, start_date, end_date, is_point_in_time

**Hypothesis**: Testable causal claims or theoretical propositions  
- Properties: description (required), prior_probability (0.0-1.0), posterior_probability (0.0-1.0), status (active/supported/partially_supported/refuted/undetermined)

**Evidence**: Facts that support/refute claims or confirm/deny events
- Properties: description (required), type (hoop/smoking_gun/straw_in_the_wind/doubly_decisive/bayesian/general), certainty (0.0-1.0), source, credibility (0.0-1.0)

**Causal_Mechanism**: Step-by-step explanations of HOW causation works
- Properties: description (required), confidence (0.0-1.0), status (hypothetical/supported/refuted/unspecified), level_of_detail

**Alternative_Explanation**: Competing theories or rival explanations
- Properties: description (required), probability (0.0-1.0), status (active/eliminated/supported/undetermined), key_predictions (ARRAY of strings, not single string)

**Actor**: People, groups, entities with agency and intentions
- Properties: description (required - use descriptive text, NOT just "name" field), name, role, intentions, beliefs, constraints, capabilities, credibility (0.0-1.0)

**Condition**: Enabling/constraining factors that affect other entities
- Properties: description (required), type (background/enabling/constraining/scope), necessity (0.0-1.0), temporal_scope, spatial_scope, certainty (0.0-1.0)

**Data_Source**: Origins of evidence (documents, witnesses, records)
- Properties: type (interview/document/observation/artifact/general), credibility (0.0-1.0), bias_risk (0.0-1.0)

## EDGE TYPES (21 types with domainâ†’range constraints):

**causes**: Event â†’ Event (causal chains)
- Properties: certainty (0.0-1.0), mechanism_id, type (direct/indirect)

**part_of_mechanism**: Event â†’ Causal_Mechanism (mechanism components)
- Properties: role, sequence_position, necessity (0.0-1.0)

**tests_hypothesis**: Evidence,Event â†’ Hypothesis (hypothesis testing)
- Properties: probative_value (0.0-1.0), test_result (ðŸš¨ VALIDATION CRITICAL: ONLY "passed"/"failed"/"ambiguous" - "supports"/"refutes" WILL CAUSE FAILURE), diagnostic_type (hoop/smoking_gun/straw_in_the_wind/doubly_decisive/general)

**tests_mechanism**: Evidence,Event â†’ Causal_Mechanism (mechanism testing)  
- Properties: probative_value (0.0-1.0), test_result (ðŸš¨ VALIDATION CRITICAL: ONLY "passed"/"failed"/"ambiguous" - "supports"/"refutes" WILL CAUSE FAILURE), diagnostic_type (hoop/smoking_gun/straw_in_the_wind/doubly_decisive/general)

**supports**: Evidence,Event â†’ Hypothesis,Event,Causal_Mechanism,Actor (positive evidential support)
- Use when: evidence/events strengthen or bolster claims, when text uses "supports", "demonstrates", "shows", "indicates", "provides evidence for", "strengthens the case that"
- Properties: probative_value (0.0-1.0), certainty (0.0-1.0), diagnostic_type (hoop/smoking_gun/straw_in_the_wind/doubly_decisive/general), target_type (event_occurrence/causal_relationship/mechanism_operation/general), source_text_quote

**refutes**: Evidence,Event â†’ Hypothesis,Event,Causal_Mechanism (refutation relationships)
- Properties: probative_value (0.0-1.0), certainty (0.0-1.0), diagnostic_type (hoop/smoking_gun/straw_in_the_wind/doubly_decisive/general), target_type (event_occurrence/causal_relationship/mechanism_operation/general), source_text_quote

**confirms_occurrence**: Evidence â†’ Event (event confirmation)
- Properties: certainty (0.0-1.0), diagnostic_type (hoop/smoking_gun/straw_in_the_wind/doubly_decisive/general), source_text_quote

**disproves_occurrence**: Evidence â†’ Event (event disproof)
- Properties: certainty (0.0-1.0), diagnostic_type (hoop/smoking_gun/straw_in_the_wind/doubly_decisive/general), source_text_quote

**explains_mechanism**: Hypothesis â†’ Causal_Mechanism (mechanism explanation)
- Properties: certainty (0.0-1.0), type_of_claim (existence/operation/significance/component_role/operational_details/unspecified)

**provides_evidence_for**: Event,Evidence â†’ Hypothesis,Causal_Mechanism,Actor,Alternative_Explanation (evidence provision)
- Properties: probative_value (0.0-1.0), reasoning, diagnostic_type (hoop/smoking_gun/straw_in_the_wind/doubly_decisive/general)

**supports_alternative**: Evidence â†’ Alternative_Explanation (alternative support)
- Properties: probative_value (0.0-1.0), certainty (0.0-1.0)

**refutes_alternative**: Evidence â†’ Alternative_Explanation (alternative refutation)
- Properties: probative_value (0.0-1.0), certainty (0.0-1.0)

**tests_alternative**: Evidence,Event â†’ Alternative_Explanation (alternative testing)
- Properties: probative_value (0.0-1.0), diagnostic_type (hoop/smoking_gun/straw_in_the_wind/doubly_decisive/general), test_result (ðŸš¨ VALIDATION CRITICAL: ONLY "passed"/"failed"/"ambiguous" - "supports"/"refutes" WILL CAUSE FAILURE)

**initiates**: Actor â†’ Event (actor agency)
- Properties: certainty (0.0-1.0), intention, agency (STRING describing actor agency, not boolean True/False)

**enables**: Condition â†’ Event,Causal_Mechanism,Hypothesis (enabling conditions)
- Properties: necessity (0.0-1.0), certainty (0.0-1.0), type

**constrains**: Condition â†’ Event,Causal_Mechanism,Actor (constraining conditions)
- Properties: certainty (0.0-1.0), type

**provides_evidence**: Data_Source â†’ Evidence (evidence sourcing)
- Properties: credibility (0.0-1.0), bias_risk (0.0-1.0), certainty (0.0-1.0)

**updates_probability**: Evidence â†’ Hypothesis (Bayesian updating)
- Properties: prior_probability (0.0-1.0), posterior_probability (0.0-1.0), Bayes_factor

**contradicts**: Evidence â†’ Evidence (evidence contradiction)
- Properties: certainty (0.0-1.0), reason

**weighs_evidence**: Evidence â†’ Evidence,Hypothesis,Causal_Mechanism (evidence quality affecting theoretical credibility)
- Use when: evidence quality impacts hypothesis/mechanism credibility ("primary sources strengthen the hypothesis", "archaeological evidence validates the mechanism", "documentary evidence tests mechanism operation", "source reliability affects theory"), also evidence-to-evidence quality comparisons ("more reliable than", "primary vs secondary source", "cross-validated", "independent confirmation")
- Properties: comparison_strength (0.0-1.0), comparison_type (stronger_than/weaker_than/equivalent_to/complements/contradicts), reasoning

**infers**: Evidence,Event,Inference_Rule â†’ Hypothesis,Causal_Mechanism (inferential reasoning from evidence/events to theory)
- Properties: certainty (0.0-1.0), logic_type

## VAN EVERA DIAGNOSTIC TYPES:
- **hoop**: Necessary but not sufficient (if absent, hypothesis fails)
- **smoking_gun**: Sufficient but not necessary (if present, hypothesis confirmed)  
- **doubly_decisive**: Both necessary and sufficient (definitive test)
- **straw_in_the_wind**: Neither necessary nor sufficient (weak evidence)

## EXTRACTION PRIORITIES:
Focus especially on these methodologically critical edge types:
- **infers**: Evidence/Events that allow logical inference to hypotheses/mechanisms (Evidenceâ†’Hypothesis, Eventâ†’Mechanism)
- **confirms_occurrence**: Evidence that establishes events actually happened (Evidenceâ†’Event)
- **refutes**: Evidence/Events that falsify hypotheses/mechanisms (Evidenceâ†’Hypothesis)
- **tests_alternative**: Evidence/Events that test competing explanations (Evidenceâ†’Alternative_Explanation)
- **weighs_evidence**: Evidence quality affecting theoretical credibility - prioritize Evidenceâ†’Hypothesis and Evidenceâ†’Mechanism relationships where source reliability/strength impacts theory validity, not just Evidenceâ†’Evidence comparisons

Extract comprehensive causal networks following these constraints. Pay special attention to domainâ†’range validity and diagnostic properties.

## REQUIRED JSON OUTPUT STRUCTURE:

You MUST return JSON with exactly this structure (use "type" not "node_type" or "edge_type"):

```json
{{
    "nodes": [
        {{
            "id": "unique_id",
            "type": "Event|Hypothesis|Evidence|Causal_Mechanism|Alternative_Explanation|Actor|Condition|Data_Source",
            "properties": {{
                "description": "ALWAYS REQUIRED - descriptive text explaining this node"
            }}
        }},
        {{
            "id": "alt_example", 
            "type": "Alternative_Explanation",
            "properties": {{
                "description": "REQUIRED descriptive text",
                "key_predictions": ["prediction 1", "prediction 2"]
            }}
        }},
        {{
            "id": "actor_example",
            "type": "Actor", 
            "properties": {{
                "description": "REQUIRED descriptive text - NOT just a name",
                "name": "Actor Name"
            }}
        }}
    ],
    "edges": [
        {{
            "id": "unique_edge_id", 
            "source_id": "source_node_id",
            "target_id": "target_node_id",
            "type": "tests_hypothesis",
            "properties": {{
                "test_result": "passed"
            }}
        }},
        {{
            "id": "test_result_example",
            "source_id": "evidence_id",
            "target_id": "mechanism_id", 
            "type": "tests_mechanism",
            "properties": {{
                "test_result": "failed"
            }}
        }},
        {{
            "id": "agency_edge_example",
            "source_id": "actor_id",
            "target_id": "event_id", 
            "type": "initiates",
            "properties": {{
                "agency": "direct intentional action"
            }}
        }}
    ]
}}
```

ðŸš¨ CRITICAL REQUIREMENTS - VALIDATION WILL FAIL IF NOT FOLLOWED:
- ALL nodes MUST have "description" property with descriptive text (NEVER omit this field - it is REQUIRED for EVERY node)
- key_predictions MUST be array of strings: ["pred1", "pred2"] 
- **test_result ENFORCEMENT**: This field MUST be exactly "passed", "failed", or "ambiguous". The following values will cause VALIDATION FAILURE and must NOT be used:
  * âŒ "supports" (FORBIDDEN - use "passed" instead)
  * âŒ "refutes" (FORBIDDEN - use "failed" instead) 
  * âŒ "inconclusive" (FORBIDDEN - use "ambiguous" instead)
  * âŒ "confirms" (FORBIDDEN - use "passed" instead)
  * âŒ "contradicts" (FORBIDDEN - use "failed" instead)
  * âŒ "validates" (FORBIDDEN - use "passed" instead)
  * âŒ Any other value (FORBIDDEN - only "passed"/"failed"/"ambiguous" allowed)
- agency MUST be descriptive string, NOT boolean true/false
- Data_Source nodes MUST include "description" field even if they also have "type" property

ðŸ” test_result MAPPING GUIDE:
- When evidence SUPPORTS/CONFIRMS a hypothesis â†’ test_result: "passed"
- When evidence REFUTES/CONTRADICTS a hypothesis â†’ test_result: "failed"  
- When evidence is UNCLEAR/MIXED about a hypothesis â†’ test_result: "ambiguous"

Text to analyze:
{text}"""

class StructuredProcessTracingExtractor:
    """
    Clean extraction using structured output instead of prompt engineering
    Eliminates manual JSON parsing and validation issues
    """
    
    def __init__(self, model_name: str = "gpt-5-mini"):
        """Initialize with direct LiteLLM for structured extraction"""
        self.model_name = model_name
        
        # Get API key - prioritize OpenAI for GPT-5-mini
        self.api_key = os.getenv("OPENAI_API_KEY") or os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
        if not self.api_key:
            raise ValueError("No API key found in environment variables. Set OPENAI_API_KEY, GOOGLE_API_KEY, or GEMINI_API_KEY")
        
    def extract_graph(self, text: str, project_name: str = "default") -> StructuredExtractionResult:
        """
        Extract process tracing graph using structured output
        
        Args:
            text: Input text to analyze
            project_name: Project identifier for metadata
            
        Returns:
            StructuredExtractionResult with graph and metadata
        """
        # PHASE 19B: Enhanced diagnostic logging for pipeline phases
        print("\n" + "="*70)
        print("PHASE 19B: DIAGNOSTIC PIPELINE EXTRACTION")
        print("="*70)
        print(f"[PIPELINE] Starting extraction with structured output (model: {self.model_name})")
        print(f"[PIPELINE] Input text size: {len(text):,} characters ({len(text)//4:,} estimated tokens)")
        print(f"[PIPELINE] Project: {project_name}")
        print(f"[PIPELINE] Timestamp: {datetime.now().isoformat()}")
        
        # Format the prompt with input text
        print(f"[PIPELINE] Formatting prompt template...")
        prompt = STRUCTURED_EXTRACTION_PROMPT.format(text=text)
        print(f"[PIPELINE] Final prompt size: {len(prompt):,} characters")
        
        try:
            # Use LiteLLM structured output with Pydantic schema
            print(f"[PIPELINE] Starting LLM extraction phase...")
            start_time = datetime.now()
            
            graph = self._extract_with_structured_output(prompt)
            
            extraction_time = (datetime.now() - start_time).total_seconds()
            print(f"[PIPELINE] âœ… Extraction completed in {extraction_time:.2f}s")
            
            # Create metadata
            print(f"[PIPELINE] Creating extraction metadata...")
            metadata = ExtractionMetadata(
                extraction_method="structured_output",
                model_used=self.model_name,
                prompt_length=len(prompt),
                text_length=len(text)
            )
            
            # Create complete result
            result = StructuredExtractionResult(
                graph=graph,
                metadata=metadata
            )
            
            # Print summary
            result.print_summary()
            
            return result
            
        except Exception as e:
            print(f"ERROR: Structured extraction failed: {str(e)}")
            print("FAIL FAST: Fix the underlying issue - no empty graph fallbacks")
            raise e
    
    def _extract_with_structured_output(self, prompt: str) -> ProcessTracingGraph:
        """Use LiteLLM structured output with Pydantic schema - fail fast if it doesn't work"""
        import time
        try:
            # PHASE 19B: Add diagnostic instrumentation for hang detection
            prompt_size_kb = len(prompt) / 1024
            print(f"[DIAGNOSTIC] Starting LiteLLM call with prompt size: {prompt_size_kb:.1f}KB")
            print(f"[DIAGNOSTIC] Model: {self.model_name}")
            print(f"[DIAGNOSTIC] Timestamp: {datetime.now().isoformat()}")
            print(f"[DIAGNOSTIC] Waiting for LLM response...")
            
            call_start_time = time.time()
            
            # Use direct LiteLLM completion with router parameters for GPT-5-mini compatibility
            # PHASE 17B: Apply exact router parameters discovered in 17A
            # PHASE 19B: Add timeout configuration for large text processing
            response = litellm.completion(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "You must respond with valid JSON following the specified schema."},
                    {"role": "user", "content": prompt}
                ],
                response_format={"type": "json_object"},
                api_key=self.api_key,
                max_completion_tokens=16384,
                # PHASE 19B: Configure timeout for large text processing (30 minutes)
                timeout=1800,  # 30 minutes in seconds
                use_in_pass_through=False,
                use_litellm_proxy=False,
                merge_reasoning_content_in_choices=False
            )
            
            call_duration = time.time() - call_start_time
            print(f"[DIAGNOSTIC] LLM call completed in {call_duration:.2f} seconds")
            
            # Get the JSON content and parse it with our Pydantic model
            json_content = response.choices[0].message.content
            if json_content:
                print(f"[DIAGNOSTIC] LLM returned {len(json_content)} characters of JSON")
                print(f"[DIAGNOSTIC] Starting JSON cleaning and validation...")
                
                # Clean and parse the JSON response
                cleaned_response = self._clean_json_response(json_content)
                print(f"[DIAGNOSTIC] JSON cleaned, starting Pydantic validation...")
                
                validation_start = time.time()
                result = ProcessTracingGraph.model_validate_json(cleaned_response)
                validation_duration = time.time() - validation_start
                
                print(f"[DIAGNOSTIC] Pydantic validation completed in {validation_duration:.2f} seconds")
                print(f"[DIAGNOSTIC] Graph extracted: {len(result.nodes)} nodes, {len(result.edges)} edges")
                
                return result
            else:
                raise ValueError("No content returned from LiteLLM")
                
        except Exception as e:
            call_duration = time.time() - call_start_time if 'call_start_time' in locals() else -1
            print(f"[ERROR] LiteLLM structured output failed after {call_duration:.2f} seconds: {str(e)}")
            print(f"[ERROR] Exception type: {type(e).__name__}")
            print(f"[ERROR] Timestamp: {datetime.now().isoformat()}")
            print("FAIL FAST: No fallbacks, no degradation - fix the LiteLLM configuration")
            raise e
    
    def _clean_json_response(self, response: str) -> str:
        """Clean JSON response from control characters and formatting issues"""
        import re
        
        # Remove control characters
        cleaned = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', response)
        
        # Remove any markdown code block markers
        cleaned = re.sub(r'^```json\s*', '', cleaned, flags=re.MULTILINE)
        cleaned = re.sub(r'^```\s*$', '', cleaned, flags=re.MULTILINE)
        
        # Strip whitespace
        cleaned = cleaned.strip()
        
        return cleaned
    
    def save_results(self, result: StructuredExtractionResult, output_path: str) -> None:
        """Save extraction results to JSON file"""
        try:
            # Convert to dict for JSON serialization
            output_data = {
                "nodes": [node.model_dump() for node in result.graph.nodes],
                "edges": [edge.model_dump() for edge in result.graph.edges],
                "metadata": result.metadata.model_dump()
            }
            
            # Save to file
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, indent=2, ensure_ascii=False, default=str)
            
            print(f"ðŸ’¾ Results saved to: {output_path}")
            
        except Exception as e:
            print(f"ERROR: Failed to save results: {str(e)}")
    
    def compare_with_current_approach(self, text: str, current_result_path: Optional[str] = None) -> Dict[str, Any]:
        """
        Compare structured output approach with current manual parsing
        Returns comparison metrics
        """
        print("ðŸ” Comparing structured vs current approach...")
        
        # Extract with structured approach
        structured_result = self.extract_graph(text)
        structured_stats = structured_result.graph.get_coverage_stats()
        
        comparison = {
            "structured_approach": {
                "node_coverage": structured_stats["nodes"]["coverage_percent"],
                "edge_coverage": structured_stats["edges"]["coverage_percent"],
                "node_types": structured_stats["nodes"]["types"],
                "edge_types": structured_stats["edges"]["types"],
                "total_nodes": structured_stats["nodes"]["total"],
                "total_edges": structured_stats["edges"]["total"]
            }
        }
        
        # Compare with current approach if available
        if current_result_path and os.path.exists(current_result_path):
            try:
                with open(current_result_path, 'r') as f:
                    current_data = json.load(f)
                
                current_node_types = list(set(node.get('type') for node in current_data.get('nodes', [])))
                current_edge_types = list(set(edge.get('type') for edge in current_data.get('edges', [])))
                
                comparison["current_approach"] = {
                    "node_coverage": (len(current_node_types) / 8) * 100,
                    "edge_coverage": (len(current_edge_types) / 21) * 100,
                    "node_types": sorted(current_node_types),
                    "edge_types": sorted(current_edge_types),
                    "total_nodes": len(current_data.get('nodes', [])),
                    "total_edges": len(current_data.get('edges', []))
                }
                
                # Calculate improvements
                comparison["improvement"] = {
                    "node_coverage_gain": comparison["structured_approach"]["node_coverage"] - comparison["current_approach"]["node_coverage"],
                    "edge_coverage_gain": comparison["structured_approach"]["edge_coverage"] - comparison["current_approach"]["edge_coverage"],
                    "new_node_types": list(set(structured_stats["nodes"]["types"]) - set(current_node_types)),
                    "new_edge_types": list(set(structured_stats["edges"]["types"]) - set(current_edge_types))
                }
                
            except Exception as e:
                print(f"âš ï¸ Could not load current results for comparison: {str(e)}")
        
        return comparison

def extract_with_structured_output(text: str, project_name: str = "test", 
                                 output_dir: str = "output_data") -> StructuredExtractionResult:
    """
    Convenience function for structured extraction
    
    Args:
        text: Input text to analyze
        project_name: Project name for output files
        output_dir: Directory for output files
        
    Returns:
        StructuredExtractionResult
    """
    # Create extractor
    extractor = StructuredProcessTracingExtractor()
    
    # Extract graph
    result = extractor.extract_graph(text, project_name)
    
    # Save results
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_path = os.path.join(output_dir, project_name, f"{project_name}_{timestamp}_structured_graph.json")
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    extractor.save_results(result, output_path)
    
    return result

if __name__ == "__main__":
    # Test the structured extractor
    test_text = """
    The Tea Act of 1773 led to the Boston Tea Party. Colonists, motivated by 
    taxation without representation, organized resistance. Samuel Adams coordinated 
    the Sons of Liberty who dumped tea into Boston Harbor. This event caused 
    King George III to pass the Intolerable Acts, which closed Boston Harbor.
    The escalating conflict eventually led to the American Revolution.
    """
    
    print("ðŸ§ª Testing structured extraction...")
    result = extract_with_structured_output(test_text, "test_structured")
    
    print("\nðŸ“Š Coverage Analysis:")
    stats = result.graph.get_coverage_stats()
    print(f"Node coverage: {stats['nodes']['coverage_percent']:.1f}% ({stats['nodes']['types_found']}/8)")
    print(f"Edge coverage: {stats['edges']['coverage_percent']:.1f}% ({stats['edges']['types_found']}/21)")
    print(f"Node types: {stats['nodes']['types']}")
    print(f"Edge types: {stats['edges']['types']}")