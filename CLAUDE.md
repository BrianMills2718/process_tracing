# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## ðŸŽ¯ CURRENT STATUS: ACADEMIC VAN EVERA METHODOLOGY IMPLEMENTATION

**System Status**: **HTML Output Quality 100% Complete** - All visualization and UX issues resolved  
**Current Priority**: **Academic Van Evera Process Tracing** - Transform technical system into rigorous scholarly analysis tool  
**Infrastructure**: **100% Complete** - All systems functional, network data rich and sufficient
**Recently Resolved**: 
- âœ… **HTML Output Quality**: All 5 critical fixes implemented and verified
- âœ… **Statistics Display**: Correct metrics (160 nodes, 203 edges)  
- âœ… **Text Truncation**: Intelligent word-boundary truncation
- âœ… **Evidence Analysis**: Enhanced graph-based extraction (8 items)
- âœ… **Causal Chain Filtering**: Reduced from 379 to 37 prioritized chains
- âœ… **UI Polish**: Clean interface, no loading spinners

**Current Analysis**: System has excellent technical foundations but implements Van Evera methodology superficially. Network data is academically sufficient (160 nodes, 203 edges, rich temporal/causal structure) but analysis lacks systematic hypothesis competition and rigorous diagnostic testing.

**Immediate Goal**: Transform from "sophisticated description system" to "rigorous theory testing system" to achieve publication-quality Van Evera process tracing analysis.

## ðŸ“‹ IMPLEMENTATION PLAN: ACADEMIC VAN EVERA METHODOLOGY

### **EVIDENCE-BASED ACADEMIC QUALITY ASSESSMENT**

**Current Academic Readiness**: **37% (Critical Deficiencies)**
**Target for Publication Quality**: **85%**
**Timeline**: **8 weeks focused development**

**Network Data Quality Analysis**:
- âœ… **Temporal Coverage**: 70% events dated (56/81 events) - SUFFICIENT
- âœ… **Causal Richness**: 191 relationships with uncertainty quantification - EXCELLENT  
- âœ… **Source Attribution**: 100% edges have probative values + source quotes - EXCELLENT
- âœ… **Scale Appropriate**: 160 nodes spanning 18+ year revolutionary period - SUFFICIENT
- âŒ **Diagnostic Imbalance**: 67% smoking gun (should be 25%) - CRITICAL FLAW
- âŒ **Missing Hoop Tests**: Only 11% (should be 25%) - BLOCKING ACADEMIC QUALITY
- âŒ **Zero Doubly Decisive**: 0% (should be 15%) - SYSTEMATIC METHODOLOGY GAP
- âŒ **Weak Theoretical Competition**: 2 alternatives (need 6-8) - PREVENTS ELIMINATION LOGIC

## Coding Philosophy (Mandatory)

### Core Development Principles
- **NO LAZY IMPLEMENTATIONS**: No mocking/stubs/fallbacks/pseudo-code/simplified implementations
- **EVIDENCE-BASED DEVELOPMENT**: All claims require raw evidence in structured evidence files  
- **ACADEMIC RIGOR FIRST**: Implement systematic hypothesis testing, not just network visualization
- **SYSTEMATIC METHODOLOGY**: Follow Van Evera's four diagnostic tests with proper Bayesian updating
- **PUBLICATION QUALITY**: Generate analysis comparable to peer-reviewed political science research

### Quality Standards  
- **Methodological Compliance**: Perfect implementation of Van Evera's diagnostic test framework
- **Theoretical Competition**: Minimum 6-8 competing hypotheses with systematic elimination logic
- **Evidential Balance**: 25% hoop, 25% smoking gun, 15% doubly decisive, 35% straw-in-wind
- **Academic Output**: Scholarly prose with systematic evaluation, not technical reports

## Project Overview

This is an LLM-enhanced Process Tracing Toolkit for advanced qualitative analysis implementing Van Evera's diagnostic methodology. The system extracts causal graphs from text and performs rigorous hypothesis testing using Bayesian probability updating and systematic evidence evaluation.

**Current Status**: Technical infrastructure complete, requires methodological restructuring for academic quality.

## ðŸŽ¯ PRIORITY 1: DIAGNOSTIC TEST REBALANCING (CRITICAL - WEEK 1-2)

### **Root Cause Analysis** 
**Evidence**: Current diagnostic distribution prevents systematic hypothesis testing:
- Smoking Gun: 67% (should be 25%) â†’ Creates confirmation bias
- Hoop Tests: 11% (should be 25%) â†’ Cannot eliminate hypotheses  
- Doubly Decisive: 0% (should be 15%) â†’ No definitive tests
- Straw in Wind: 22% (should be 35%) â†’ Limited suggestive evidence

**Academic Impact**: Cannot perform rigorous elimination logic or systematic theory testing.

### **SOLUTION IMPLEMENTATION**
**File to Create**: `core/diagnostic_rebalancer.py`

**IMPLEMENT COMPLETE MODULE**:
```python
"""
Van Evera Diagnostic Test Rebalancing System
Transforms existing evidence to achieve academic distribution standards
"""

import json
from typing import Dict, List, Tuple
from core.structured_models import EvidenceAssessment
from core.enhance_evidence import refine_evidence_assessment_with_llm

class VanEveraDiagnosticRebalancer:
    """
    Rebalances evidence diagnostic types to meet academic Van Evera standards.
    Target Distribution: 25% hoop, 25% smoking_gun, 15% doubly_decisive, 35% straw_in_wind
    """
    
    TARGET_DISTRIBUTION = {
        'hoop': 0.25,           # Necessary but not sufficient
        'smoking_gun': 0.25,    # Sufficient but not necessary  
        'doubly_decisive': 0.15, # Both necessary and sufficient
        'straw_in_wind': 0.35   # Neither necessary nor sufficient
    }
    
    def __init__(self, graph_data: Dict):
        self.graph_data = graph_data
        self.evidence_nodes = [n for n in graph_data['nodes'] if n.get('type') == 'Evidence']
        self.hypothesis_nodes = [n for n in graph_data['nodes'] if n.get('type') == 'Hypothesis']
        self.evidence_edges = [e for e in graph_data['edges'] if self._is_evidence_edge(e)]
        
    def _is_evidence_edge(self, edge: Dict) -> bool:
        """Check if edge connects evidence to hypothesis"""
        source_node = next((n for n in self.graph_data['nodes'] if n['id'] == edge['source_id']), None)
        target_node = next((n for n in self.graph_data['nodes'] if n['id'] == edge['target_id']), None)
        return (source_node and source_node.get('type') == 'Evidence' and 
                target_node and target_node.get('type') == 'Hypothesis')
    
    def analyze_current_distribution(self) -> Dict:
        """Analyze current diagnostic type distribution"""
        distribution = {'hoop': 0, 'smoking_gun': 0, 'doubly_decisive': 0, 'straw_in_wind': 0, 'general': 0}
        
        for edge in self.evidence_edges:
            diagnostic_type = edge.get('properties', {}).get('diagnostic_type', 'general')
            if diagnostic_type in distribution:
                distribution[diagnostic_type] += 1
            else:
                distribution['general'] += 1
        
        total = sum(distribution.values())
        if total > 0:
            percentages = {k: v/total for k, v in distribution.items()}
        else:
            percentages = distribution
            
        return {
            'counts': distribution,
            'percentages': percentages,
            'total_evidence': total,
            'deviation_from_target': self._calculate_deviation(percentages)
        }
    
    def _calculate_deviation(self, current_percentages: Dict) -> Dict:
        """Calculate deviation from target academic distribution"""
        deviation = {}
        for test_type, target_pct in self.TARGET_DISTRIBUTION.items():
            current_pct = current_percentages.get(test_type, 0)
            deviation[test_type] = {
                'current': current_pct,
                'target': target_pct,
                'gap': target_pct - current_pct,
                'needs_increase': target_pct > current_pct
            }
        return deviation
    
    def rebalance_diagnostics(self, query_llm_func=None) -> Dict:
        """
        Rebalance diagnostic types using LLM assessment.
        Returns updated graph data with proper diagnostic distribution.
        """
        print("[DIAGNOSTIC_REBALANCE] Starting Van Evera diagnostic rebalancing...")
        
        current_analysis = self.analyze_current_distribution()
        print(f"[DIAGNOSTIC_REBALANCE] Current distribution: {current_analysis['percentages']}")
        print(f"[DIAGNOSTIC_REBALANCE] Target distribution: {self.TARGET_DISTRIBUTION}")
        
        # Get evidence that needs reclassification
        reclassification_needed = self._identify_reclassification_candidates(current_analysis)
        
        updated_edges = []
        rebalance_stats = {'reclassified': 0, 'enhanced': 0, 'errors': 0}
        
        for edge in self.evidence_edges:
            edge_id = f"{edge['source_id']}->{edge['target_id']}"
            
            if edge_id in reclassification_needed:
                # Use LLM to reassess diagnostic type
                enhanced_edge = self._enhance_evidence_edge(edge, query_llm_func)
                if enhanced_edge:
                    updated_edges.append(enhanced_edge)
                    rebalance_stats['reclassified'] += 1
                else:
                    updated_edges.append(edge)  # Keep original if enhancement fails
                    rebalance_stats['errors'] += 1
            else:
                updated_edges.append(edge)
        
        # Update graph data with rebalanced edges
        updated_graph = self.graph_data.copy()
        non_evidence_edges = [e for e in self.graph_data['edges'] if not self._is_evidence_edge(e)]
        updated_graph['edges'] = non_evidence_edges + updated_edges
        
        # Verify final distribution
        final_analysis = self._analyze_final_distribution(updated_edges)
        
        print(f"[DIAGNOSTIC_REBALANCE] Rebalancing complete:")
        print(f"  - Reclassified: {rebalance_stats['reclassified']} evidence items")
        print(f"  - Final distribution: {final_analysis['percentages']}")
        print(f"  - Academic compliance: {final_analysis['academic_compliance_score']}%")
        
        return {
            'updated_graph_data': updated_graph,
            'rebalance_statistics': rebalance_stats,
            'before_distribution': current_analysis,
            'after_distribution': final_analysis
        }
    
    def _identify_reclassification_candidates(self, current_analysis: Dict) -> List[str]:
        """Identify which evidence edges need diagnostic reclassification"""
        deviation = current_analysis['deviation_from_target']
        candidates = []
        
        # Prioritize reclassifying over-represented types to under-represented ones
        over_represented = [k for k, v in deviation.items() if v['gap'] < -0.05]  # >5% over target
        under_represented = [k for k, v in deviation.items() if v['gap'] > 0.05]   # >5% under target
        
        if over_represented and under_represented:
            # Select candidates from over-represented categories
            for edge in self.evidence_edges:
                current_type = edge.get('properties', {}).get('diagnostic_type', 'general')
                if current_type in over_represented or current_type == 'general':
                    edge_id = f"{edge['source_id']}->{edge['target_id']}"
                    candidates.append(edge_id)
        
        # Limit to reasonable batch size for LLM processing
        return candidates[:min(len(candidates), 20)]
    
    def _enhance_evidence_edge(self, edge: Dict, query_llm_func) -> Dict:
        """Use LLM to reassess and enhance diagnostic type for evidence edge"""
        try:
            # Get evidence and hypothesis descriptions
            evidence_node = next((n for n in self.graph_data['nodes'] if n['id'] == edge['source_id']), None)
            hypothesis_node = next((n for n in self.graph_data['nodes'] if n['id'] == edge['target_id']), None)
            
            if not evidence_node or not hypothesis_node:
                return None
                
            evidence_desc = evidence_node.get('properties', {}).get('description', edge['source_id'])
            hypothesis_desc = hypothesis_node.get('properties', {}).get('description', edge['target_id'])
            
            context_info = f"Hypothesis: {hypothesis_desc}\nEvidence: {evidence_desc}"
            
            # Use LLM enhancement with Van Evera focus
            enhanced_assessment = refine_evidence_assessment_with_llm(
                evidence_description=evidence_desc,
                text_content=context_info,
                context_info=f"Van Evera diagnostic rebalancing for hypothesis: {hypothesis_desc}",
                query_llm_func=query_llm_func
            )
            
            if enhanced_assessment and enhanced_assessment.diagnostic_type:
                # Update edge with new diagnostic type
                updated_edge = edge.copy()
                if 'properties' not in updated_edge:
                    updated_edge['properties'] = {}
                
                updated_edge['properties']['diagnostic_type'] = enhanced_assessment.diagnostic_type
                updated_edge['properties']['probative_value'] = enhanced_assessment.probative_value
                updated_edge['properties']['llm_enhanced'] = True
                updated_edge['properties']['enhancement_timestamp'] = str(datetime.utcnow())
                
                return updated_edge
                
        except Exception as e:
            print(f"[DIAGNOSTIC_REBALANCE] Error enhancing edge {edge.get('source_id', '?')}->{edge.get('target_id', '?')}: {e}")
            
        return None
    
    def _analyze_final_distribution(self, updated_edges: List[Dict]) -> Dict:
        """Analyze final diagnostic distribution after rebalancing"""
        distribution = {'hoop': 0, 'smoking_gun': 0, 'doubly_decisive': 0, 'straw_in_wind': 0}
        
        for edge in updated_edges:
            diagnostic_type = edge.get('properties', {}).get('diagnostic_type', 'general')
            if diagnostic_type in distribution:
                distribution[diagnostic_type] += 1
        
        total = sum(distribution.values())
        percentages = {k: v/total for k, v in distribution.items()} if total > 0 else distribution
        
        # Calculate academic compliance score
        compliance_score = 0
        for test_type, target_pct in self.TARGET_DISTRIBUTION.items():
            actual_pct = percentages.get(test_type, 0)
            # Score based on proximity to target (100% = perfect match)
            test_score = max(0, 100 - abs(target_pct - actual_pct) * 200)
            compliance_score += test_score * 0.25  # Weight equally
        
        return {
            'counts': distribution,
            'percentages': percentages,
            'total_evidence': total,
            'academic_compliance_score': round(compliance_score, 1)
        }

# Integration function for main analysis pipeline
def rebalance_van_evera_diagnostics(graph_data: Dict, query_llm_func=None) -> Dict:
    """
    Main entry point for diagnostic rebalancing.
    Returns updated graph data with academic Van Evera distribution.
    """
    rebalancer = VanEveraDiagnosticRebalancer(graph_data)
    result = rebalancer.rebalance_diagnostics(query_llm_func)
    return result['updated_graph_data']
```

**Integration Point**: `core/analyze.py` 
**Add after line ~2500 in format_html_analysis function**:
```python
# Import at top of file
from core.diagnostic_rebalancer import rebalance_van_evera_diagnostics

# Add before HTML generation
if 'graph_data' in results:
    print("[ANALYSIS] Rebalancing Van Evera diagnostic tests for academic quality...")
    results['graph_data'] = rebalance_van_evera_diagnostics(
        results['graph_data'], 
        query_llm_func=lambda text, **kwargs: query_llm(text, **kwargs)
    )
```

**Validation**:
```bash
python process_trace_advanced.py --project revolutions --analyze-only --graph-file "output_data/revolutions/revolutions_20250805_122000_graph.json"
grep -o "diagnostic_type.*hoop\|diagnostic_type.*smoking_gun" output_data/revolutions/revolutions_*_graph.json | wc -l
# Expected: More balanced distribution (not 67% smoking gun)
```

## ðŸŽ¯ PRIORITY 2: ALTERNATIVE HYPOTHESIS GENERATION (CRITICAL - WEEK 2-3)

### **Root Cause Analysis**
**Evidence**: Current system has only 2 weak alternative explanations for entire American Revolution:
- `Alternative_Explanation` nodes: 2 total
- Required for academic rigor: 6-8 robust competing hypotheses
- Impact: Cannot perform systematic elimination logic or comparative theory testing

**Academic Standard**: Van Evera requires testing multiple competing explanations simultaneously to strengthen causal claims through elimination.

### **SOLUTION IMPLEMENTATION** 
**File to Create**: `core/alternative_hypothesis_generator.py`

**IMPLEMENT COMPLETE MODULE**:
```python
"""
Alternative Hypothesis Generation System
Creates systematic theoretical competition for rigorous Van Evera analysis
"""

from typing import Dict, List
import json

class AlternativeHypothesisGenerator:
    """
    Generates competing explanations for systematic Van Evera hypothesis testing.
    Creates 6-8 robust alternative hypotheses across multiple theoretical domains.
    """
    
    # Academic-quality alternative explanations for American Revolution
    REVOLUTION_ALTERNATIVE_HYPOTHESES = {
        'economic_interests': {
            'description': "Colonial merchant class drove resistance to protect trade profits and economic autonomy from British mercantile restrictions",
            'key_predictions': [
                "Resistance leaders predominantly merchants/traders",
                "Opposition correlates with trade disruption severity",
                "Economic arguments dominate political rhetoric",
                "Regional variation based on trade dependencies"
            ],
            'theoretical_basis': "Economic determinism, rational choice theory",
            'testable_mechanisms': [
                "Merchant wealth â†’ Political influence â†’ Resistance leadership",
                "Trade restrictions â†’ Economic losses â†’ Opposition mobilization",
                "Credit relationships â†’ Cross-colonial coordination"
            ]
        },
        
        'generational_conflict': {
            'description': "Young colonial generation rejected parental authority and British rule as part of broader generational rebellion",
            'key_predictions': [
                "Resistance leaders significantly younger than average",
                "Generational rhetoric in political arguments",
                "Young men disproportionately involved in crowd actions",
                "Parent-child political divisions documented"
            ],
            'theoretical_basis': "Generational theory, political socialization",
            'testable_mechanisms': [
                "Demographic bulge â†’ Social pressure â†’ Political radicalization",
                "Educational changes â†’ New ideas â†’ Generational divergence",
                "Military service â†’ Independence experience â†’ Authority rejection"
            ]
        },
        
        'religious_awakening': {
            'description': "Protestant religious awakening created ideological framework for independence from Anglican/secular British authority",
            'key_predictions': [
                "Religious language permeates political documents",
                "Clergy provide intellectual leadership for resistance", 
                "Regional correlation between religious revival and political opposition",
                "Anti-Anglican sentiment drives anti-British sentiment"
            ],
            'theoretical_basis': "Religious sociology, ideological mobilization theory",
            'testable_mechanisms': [
                "Religious revival â†’ Authority questioning â†’ Political resistance",
                "Clergy networks â†’ Communication channels â†’ Coordinated opposition",
                "Moral arguments â†’ Popular legitimacy â†’ Mass mobilization"
            ]
        },
        
        'elite_power_struggle': {
            'description': "Colonial political elites sought to replace British dominance with their own power, using popular grievances instrumentally",
            'key_predictions': [
                "Elite continuity before/after revolution",
                "Popular movements controlled/channeled by elites",
                "Elite economic interests protected in new system",
                "Limited democratic expansion post-independence"
            ],
            'theoretical_basis': "Elite theory, power transition models",
            'testable_mechanisms': [
                "Elite competition â†’ Popular mobilization â†’ Elite victory",
                "British weakness â†’ Elite opportunity â†’ Power grab",
                "Popular grievances â†’ Elite manipulation â†’ Controlled revolution"
            ]
        },
        
        'regional_political_culture': {
            'description': "New England political culture of town meetings and local self-governance was fundamentally incompatible with monarchical authority",
            'key_predictions': [
                "Regional variation in resistance intensity",
                "Local governance experience predicts opposition",
                "Cultural arguments about political participation",
                "Institutional legacy shapes post-war governance"
            ],
            'theoretical_basis': "Political culture theory, institutional analysis",
            'testable_mechanisms': [
                "Local democracy â†’ Democratic values â†’ Anti-monarchical attitudes",
                "Institutional experience â†’ Governance capacity â†’ Independence viability",
                "Cultural transmission â†’ Persistent regional differences"
            ]
        },
        
        'imperial_overstretch': {
            'description': "British administrative incompetence and imperial overextension created governance failures that generated resistance",
            'key_predictions': [
                "British policy inconsistency and frequent reversals",
                "Administrative failures precede colonial resistance",
                "Distance/communication delays create policy problems",
                "Similar problems in other British colonies"
            ],
            'theoretical_basis': "Imperial decline theory, administrative capacity",
            'testable_mechanisms': [
                "Geographic distance â†’ Communication delays â†’ Policy failures",
                "Multiple commitments â†’ Resource constraints â†’ Governance breakdown", 
                "Bureaucratic incompetence â†’ Arbitrary decisions â†’ Popular grievances"
            ]
        },
        
        'military_catalyst': {
            'description': "French and Indian War transformed colonial military experience and created veteran leadership that enabled resistance",
            'key_predictions': [
                "Military veterans disproportionately lead resistance",
                "Military organization patterns in resistance groups",
                "Strategic thinking evident in colonial planning",
                "Military precedents for challenging British authority"
            ],
            'theoretical_basis': "Military sociology, veteran political mobilization",
            'testable_mechanisms': [
                "Military service â†’ Leadership skills â†’ Political leadership",
                "Veteran networks â†’ Communication channels â†’ Coordinated resistance",
                "Military confidence â†’ Resistance feasibility â†’ Revolutionary commitment"
            ]
        },
        
        'ideological_contagion': {
            'description': "Enlightenment ideas about natural rights and popular sovereignty spread through colonial intellectual networks and newspapers",
            'key_predictions': [
                "Philosophical arguments dominate political documents",
                "Intellectual networks connect resistance leaders",
                "Newspaper circulation correlates with political mobilization",
                "European intellectual influences clearly documented"
            ],
            'theoretical_basis': "Ideational diffusion theory, intellectual history",
            'testable_mechanisms': [
                "Print culture â†’ Idea transmission â†’ Political mobilization",
                "Intellectual networks â†’ Coordinated arguments â†’ Movement coherence",
                "Educational institutions â†’ Elite formation â†’ Leadership development"
            ]
        }
    }
    
    def __init__(self, graph_data: Dict):
        self.graph_data = graph_data
        self.existing_hypotheses = [n for n in graph_data['nodes'] if n.get('type') == 'Hypothesis']
        self.existing_alternatives = [n for n in graph_data['nodes'] if n.get('type') == 'Alternative_Explanation']
        
    def generate_comprehensive_alternatives(self) -> Dict:
        """
        Generate comprehensive set of alternative hypotheses for systematic testing.
        Returns updated graph data with robust theoretical competition.
        """
        print("[ALTERNATIVE_GENERATION] Creating systematic theoretical competition...")
        
        new_nodes = []
        new_edges = []
        hypothesis_counter = len(self.existing_hypotheses) + len(self.existing_alternatives)
        
        for alt_id, alt_data in self.REVOLUTION_ALTERNATIVE_HYPOTHESES.items():
            # Create alternative hypothesis node
            hypothesis_counter += 1
            node_id = f"ALT_H_{hypothesis_counter:03d}"
            
            alternative_node = {
                'id': node_id,
                'type': 'Alternative_Explanation',
                'properties': {
                    'description': alt_data['description'],
                    'theoretical_basis': alt_data['theoretical_basis'],
                    'key_predictions': alt_data['key_predictions'],
                    'testable_mechanisms': alt_data['testable_mechanisms'],
                    'generated_by': 'alternative_hypothesis_generator',
                    'academic_quality': True
                }
            }
            new_nodes.append(alternative_node)
            
            # Create competitive relationships with existing hypotheses
            for existing_hyp in self.existing_hypotheses:
                # Add competition edge
                competition_edge = {
                    'source_id': node_id,
                    'target_id': existing_hyp['id'],
                    'type': 'competes_with',
                    'properties': {
                        'relationship_type': 'theoretical_competition',
                        'competition_strength': 0.8,
                        'testable_difference': True,
                        'elimination_potential': True
                    }
                }
                new_edges.append(competition_edge)
            
            # Connect to relevant evidence that could test this alternative
            relevant_evidence = self._identify_relevant_evidence(alt_data)
            for evidence_node in relevant_evidence:
                evidence_edge = {
                    'source_id': evidence_node['id'],
                    'target_id': node_id,
                    'type': 'potentially_supports',
                    'properties': {
                        'diagnostic_type': 'straw_in_wind',  # Default, will be refined
                        'probative_value': 0.6,
                        'requires_testing': True,
                        'theoretical_relevance': alt_data['theoretical_basis']
                    }
                }
                new_edges.append(evidence_edge)
        
        # Update graph data
        updated_graph = self.graph_data.copy()
        updated_graph['nodes'].extend(new_nodes)
        updated_graph['edges'].extend(new_edges)
        
        print(f"[ALTERNATIVE_GENERATION] Generated {len(new_nodes)} alternative hypotheses")
        print(f"[ALTERNATIVE_GENERATION] Created {len(new_edges)} new evidence/competition relationships")
        print(f"[ALTERNATIVE_GENERATION] Total alternative explanations: {len(new_nodes) + len(self.existing_alternatives)}")
        
        return updated_graph
    
    def _identify_relevant_evidence(self, alt_data: Dict) -> List[Dict]:
        """
        Identify evidence nodes that could potentially test this alternative hypothesis.
        Uses keyword matching and thematic relevance.
        """
        relevant_evidence = []
        evidence_nodes = [n for n in self.graph_data['nodes'] if n.get('type') == 'Evidence']
        
        # Extract keywords from alternative hypothesis
        description = alt_data['description'].lower()
        predictions = ' '.join(alt_data['key_predictions']).lower()
        search_text = f"{description} {predictions}"
        
        # Keyword matching for relevance
        relevance_keywords = {
            'economic': ['merchant', 'trade', 'profit', 'economic', 'commercial', 'business', 'money'],
            'generational': ['young', 'generation', 'age', 'youth', 'elder', 'father', 'son'],
            'religious': ['religious', 'church', 'clergy', 'god', 'christian', 'protestant', 'anglican'],
            'elite': ['elite', 'leader', 'wealthy', 'class', 'aristocrat', 'gentleman'],
            'cultural': ['culture', 'tradition', 'local', 'community', 'custom'],
            'military': ['military', 'war', 'soldier', 'veteran', 'army', 'battle'],
            'ideological': ['idea', 'philosophy', 'enlightenment', 'rights', 'liberty', 'freedom'],
            'administrative': ['administration', 'policy', 'government', 'bureaucrat', 'official']
        }
        
        for evidence_node in evidence_nodes:
            evidence_desc = evidence_node.get('properties', {}).get('description', '').lower()
            
            # Check for keyword overlap
            relevance_score = 0
            for theme_keywords in relevance_keywords.values():
                for keyword in theme_keywords:
                    if keyword in search_text and keyword in evidence_desc:
                        relevance_score += 1
            
            # Include evidence with sufficient relevance
            if relevance_score >= 2:  # Threshold for relevance
                relevant_evidence.append(evidence_node)
        
        return relevant_evidence[:10]  # Limit to top 10 most relevant pieces of evidence

# Integration function
def generate_alternative_hypotheses(graph_data: Dict) -> Dict:
    """
    Main entry point for alternative hypothesis generation.
    Returns updated graph data with comprehensive theoretical competition.
    """
    generator = AlternativeHypothesisGenerator(graph_data)
    return generator.generate_comprehensive_alternatives()
```

**Integration Point**: `core/analyze.py`
**Add after diagnostic rebalancing**:
```python
# Import at top
from core.alternative_hypothesis_generator import generate_alternative_hypotheses

# Add after diagnostic rebalancing
if 'graph_data' in results:
    print("[ANALYSIS] Generating alternative hypotheses for systematic testing...")
    results['graph_data'] = generate_alternative_hypotheses(results['graph_data'])
```

**Validation**:
```bash
python process_trace_advanced.py --project revolutions --analyze-only --graph-file "output_data/revolutions/revolutions_20250805_122000_graph.json"
python -c "
import json
data = json.load(open('output_data/revolutions/revolutions_*_graph.json'))
alternatives = [n for n in data['nodes'] if n['type'] == 'Alternative_Explanation']
print(f'Alternative explanations: {len(alternatives)}')
"
# Expected: 8-10 alternative explanations (not just 2)
```

## ðŸŽ¯ PRIORITY 3: SYSTEMATIC HYPOTHESIS TESTING ENGINE (CRITICAL - WEEK 3-5)

### **Root Cause Analysis**
**Evidence**: Current system shows evidence â†’ hypothesis relationships but doesn't implement systematic Van Evera testing logic:
- No hypothesis â†’ prediction â†’ test â†’ elimination sequence
- No Bayesian probability updating 
- No comparative evaluation across competing theories
- Missing "did hypothesis pass/fail test?" conclusions

**Academic Requirement**: Van Evera's methodology requires systematic testing of specific predictions derived from each hypothesis, with clear pass/fail determinations and elimination logic.

### **SOLUTION IMPLEMENTATION**
**File to Create**: `core/van_evera_testing_engine.py`

**IMPLEMENT COMPLETE MODULE**:
```python
"""
Van Evera Systematic Hypothesis Testing Engine
Implements rigorous academic process tracing methodology with Bayesian updating
"""

from typing import Dict, List, Tuple, Optional
import json
import math
from dataclasses import dataclass
from enum import Enum

class TestResult(Enum):
    PASS = "PASS"
    FAIL = "FAIL" 
    INCONCLUSIVE = "INCONCLUSIVE"

class DiagnosticType(Enum):
    HOOP = "hoop"                    # Necessary but not sufficient
    SMOKING_GUN = "smoking_gun"      # Sufficient but not necessary  
    DOUBLY_DECISIVE = "doubly_decisive"  # Both necessary and sufficient
    STRAW_IN_WIND = "straw_in_wind"  # Neither necessary nor sufficient

@dataclass
class TestPrediction:
    """Specific testable prediction derived from hypothesis"""
    prediction_id: str
    hypothesis_id: str
    description: str
    diagnostic_type: DiagnosticType
    necessary_condition: bool
    sufficient_condition: bool
    evidence_requirements: List[str]

@dataclass 
class TestEvaluation:
    """Result of testing a specific prediction"""
    prediction_id: str
    test_result: TestResult
    supporting_evidence: List[str]
    contradicting_evidence: List[str]
    confidence_level: float
    reasoning: str
    elimination_implications: List[str]

@dataclass
class HypothesisAssessment:
    """Complete assessment of hypothesis after all tests"""
    hypothesis_id: str
    description: str
    prior_probability: float
    posterior_probability: float
    test_results: List[TestEvaluation]
    overall_status: str  # "SUPPORTED", "ELIMINATED", "WEAKENED", "INCONCLUSIVE"
    confidence_interval: Tuple[float, float]
    academic_conclusion: str

class VanEveraTestingEngine:
    """
    Implements systematic Van Evera hypothesis testing with academic rigor.
    Performs Bayesian updating and comparative elimination logic.
    """
    
    def __init__(self, graph_data: Dict):
        self.graph_data = graph_data
        self.hypotheses = [n for n in graph_data['nodes'] 
                          if n.get('type') in ['Hypothesis', 'Alternative_Explanation']]
        self.evidence = [n for n in graph_data['nodes'] if n.get('type') == 'Evidence']
        self.evidence_edges = [e for e in graph_data['edges'] if self._is_evidence_relationship(e)]
        
    def _is_evidence_relationship(self, edge: Dict) -> bool:
        """Check if edge represents evidence-hypothesis relationship"""
        source_node = next((n for n in self.graph_data['nodes'] if n['id'] == edge['source_id']), None)
        target_node = next((n for n in self.graph_data['nodes'] if n['id'] == edge['target_id']), None)
        return (source_node and source_node.get('type') == 'Evidence' and 
                target_node and target_node.get('type') in ['Hypothesis', 'Alternative_Explanation'])
    
    def generate_testable_predictions(self, hypothesis: Dict) -> List[TestPrediction]:
        """
        Generate specific, testable predictions from hypothesis.
        Academic Van Evera requires explicit predictions that can be systematically tested.
        """
        hypothesis_id = hypothesis['id']
        hypothesis_desc = hypothesis.get('properties', {}).get('description', '')
        
        # Extract domain-specific predictions based on hypothesis content
        predictions = []
        
        if 'taxation without representation' in hypothesis_desc.lower():
            predictions.extend([
                TestPrediction(
                    prediction_id=f"{hypothesis_id}_PRED_001",
                    hypothesis_id=hypothesis_id,
                    description="Colonial resistance rhetoric must consistently invoke English constitutional rights",
                    diagnostic_type=DiagnosticType.HOOP,
                    necessary_condition=True,
                    sufficient_condition=False,
                    evidence_requirements=["legal_arguments", "constitutional_rhetoric", "rights_language"]
                ),
                TestPrediction(
                    prediction_id=f"{hypothesis_id}_PRED_002", 
                    hypothesis_id=hypothesis_id,
                    description="Opposition intensity should correlate with tax burden increases",
                    diagnostic_type=DiagnosticType.SMOKING_GUN,
                    necessary_condition=False,
                    sufficient_condition=True,
                    evidence_requirements=["tax_legislation", "resistance_timing", "burden_measurement"]
                )
            ])
        
        elif 'ideological' in hypothesis_desc.lower() and 'political' in hypothesis_desc.lower():
            predictions.extend([
                TestPrediction(
                    prediction_id=f"{hypothesis_id}_PRED_001",
                    hypothesis_id=hypothesis_id,
                    description="Political documents must contain systematic philosophical arguments",
                    diagnostic_type=DiagnosticType.HOOP,
                    necessary_condition=True, 
                    sufficient_condition=False,
                    evidence_requirements=["philosophical_language", "systematic_arguments", "intellectual_references"]
                ),
                TestPrediction(
                    prediction_id=f"{hypothesis_id}_PRED_002",
                    hypothesis_id=hypothesis_id,
                    description="Revolutionary leadership should demonstrate intellectual sophistication",
                    diagnostic_type=DiagnosticType.STRAW_IN_WIND,
                    necessary_condition=False,
                    sufficient_condition=False,
                    evidence_requirements=["leader_education", "intellectual_networks", "philosophical_writings"]
                )
            ])
        
        elif 'self-governance' in hypothesis_desc.lower():
            predictions.extend([
                TestPrediction(
                    prediction_id=f"{hypothesis_id}_PRED_001",
                    hypothesis_id=hypothesis_id,
                    description="Resistance must emerge from established local governance institutions",
                    diagnostic_type=DiagnosticType.DOUBLY_DECISIVE,
                    necessary_condition=True,
                    sufficient_condition=True,
                    evidence_requirements=["local_institutions", "institutional_continuity", "governance_experience"]
                ),
                TestPrediction(
                    prediction_id=f"{hypothesis_id}_PRED_002",
                    hypothesis_id=hypothesis_id,
                    description="Post-1763 British policies must directly threaten local autonomy",
                    diagnostic_type=DiagnosticType.HOOP,
                    necessary_condition=True,
                    sufficient_condition=False,
                    evidence_requirements=["policy_changes", "autonomy_threats", "local_responses"]
                )
            ])
        
        # For alternative explanations, generate competing predictions
        elif hypothesis.get('type') == 'Alternative_Explanation':
            alt_desc = hypothesis_desc.lower()
            if 'merchant' in alt_desc or 'economic' in alt_desc:
                predictions.extend([
                    TestPrediction(
                        prediction_id=f"{hypothesis_id}_PRED_001",
                        hypothesis_id=hypothesis_id,
                        description="Resistance leaders must be predominantly merchants or have merchant connections",
                        diagnostic_type=DiagnosticType.SMOKING_GUN,
                        necessary_condition=False,
                        sufficient_condition=True,
                        evidence_requirements=["leader_occupations", "merchant_networks", "economic_interests"]
                    ),
                    TestPrediction(
                        prediction_id=f"{hypothesis_id}_PRED_002",
                        hypothesis_id=hypothesis_id,
                        description="Opposition timing must correlate with trade disruption severity",
                        diagnostic_type=DiagnosticType.HOOP,
                        necessary_condition=True,
                        sufficient_condition=False,
                        evidence_requirements=["trade_data", "disruption_timing", "resistance_timing"]
                    )
                ])
            
            elif 'religious' in alt_desc or 'awakening' in alt_desc:
                predictions.extend([
                    TestPrediction(
                        prediction_id=f"{hypothesis_id}_PRED_001",
                        hypothesis_id=hypothesis_id,
                        description="Political rhetoric must contain systematic religious/moral language",
                        diagnostic_type=DiagnosticType.SMOKING_GUN,
                        necessary_condition=False,
                        sufficient_condition=True,
                        evidence_requirements=["religious_rhetoric", "moral_arguments", "clerical_leadership"]
                    ),
                    TestPrediction(
                        prediction_id=f"{hypothesis_id}_PRED_002",
                        hypothesis_id=hypothesis_id,
                        description="Regional religious revival intensity must correlate with political resistance",
                        diagnostic_type=DiagnosticType.HOOP,
                        necessary_condition=True,
                        sufficient_condition=False,
                        evidence_requirements=["religious_data", "revival_timing", "regional_variation"]
                    )
                ])
        
        return predictions
    
    def evaluate_prediction(self, prediction: TestPrediction) -> TestEvaluation:
        """
        Systematically evaluate whether prediction passes or fails based on available evidence.
        Implements Van Evera's logic for each diagnostic test type.
        """
        # Find relevant evidence for this prediction
        relevant_evidence = []
        contradicting_evidence = []
        
        for edge in self.evidence_edges:
            if edge['target_id'] == prediction.hypothesis_id:
                evidence_node = next((n for n in self.graph_data['nodes'] 
                                    if n['id'] == edge['source_id']), None)
                if evidence_node:
                    evidence_desc = evidence_node.get('properties', {}).get('description', '').lower()
                    edge_type = edge.get('type', '')
                    
                    # Check if evidence relates to this prediction
                    prediction_keywords = self._extract_prediction_keywords(prediction)
                    if any(keyword in evidence_desc for keyword in prediction_keywords):
                        if edge_type in ['supports', 'provides_evidence_for']:
                            relevant_evidence.append(evidence_node['id'])
                        elif edge_type in ['refutes', 'contradicts']:
                            contradicting_evidence.append(evidence_node['id'])
        
        # Apply Van Evera diagnostic logic
        test_result, reasoning = self._apply_diagnostic_logic(
            prediction, relevant_evidence, contradicting_evidence
        )
        
        # Calculate confidence level based on evidence strength
        confidence = self._calculate_confidence(relevant_evidence, contradicting_evidence, prediction)
        
        # Determine elimination implications
        elimination_implications = self._assess_elimination_implications(prediction, test_result)
        
        return TestEvaluation(
            prediction_id=prediction.prediction_id,
            test_result=test_result,
            supporting_evidence=relevant_evidence,
            contradicting_evidence=contradicting_evidence,
            confidence_level=confidence,
            reasoning=reasoning,
            elimination_implications=elimination_implications
        )
    
    def _extract_prediction_keywords(self, prediction: TestPrediction) -> List[str]:
        """Extract keywords from prediction for evidence matching"""
        desc = prediction.description.lower()
        keywords = []
        
        # Extract key terms based on prediction content
        if 'rights' in desc or 'constitutional' in desc:
            keywords.extend(['rights', 'constitutional', 'liberty', 'freedom', 'magna carta'])
        if 'merchant' in desc or 'trade' in desc:
            keywords.extend(['merchant', 'trade', 'commercial', 'business', 'profit'])
        if 'religious' in desc or 'moral' in desc:
            keywords.extend(['religious', 'god', 'christian', 'clergy', 'moral'])
        if 'governance' in desc or 'institutional' in desc:
            keywords.extend(['governance', 'institution', 'assembly', 'government'])
        if 'correlation' in desc or 'timing' in desc:
            keywords.extend(['timing', 'correlation', 'relationship', 'pattern'])
            
        return keywords
    
    def _apply_diagnostic_logic(self, prediction: TestPrediction, 
                               supporting: List[str], contradicting: List[str]) -> Tuple[TestResult, str]:
        """Apply Van Evera diagnostic test logic"""
        has_support = len(supporting) > 0
        has_contradiction = len(contradicting) > 0
        
        if prediction.diagnostic_type == DiagnosticType.HOOP:
            # Necessary condition - must pass to remain viable
            if has_contradiction or not has_support:
                return TestResult.FAIL, f"HOOP TEST FAILED: Hypothesis eliminated. Required evidence not found or contradicted. Supporting: {len(supporting)}, Contradicting: {len(contradicting)}"
            else:
                return TestResult.PASS, f"HOOP TEST PASSED: Hypothesis remains viable. Necessary condition satisfied. Supporting evidence: {len(supporting)}"
        
        elif prediction.diagnostic_type == DiagnosticType.SMOKING_GUN:
            # Sufficient condition - if passes, strongly confirms hypothesis
            if has_support and not has_contradiction:
                return TestResult.PASS, f"SMOKING GUN PASSED: Hypothesis strongly confirmed. Sufficient evidence found. Supporting: {len(supporting)}"
            elif has_contradiction:
                return TestResult.FAIL, f"SMOKING GUN FAILED: Evidence contradicts prediction. Contradicting: {len(contradicting)}"
            else:
                return TestResult.INCONCLUSIVE, f"SMOKING GUN INCONCLUSIVE: No decisive evidence found. Neither confirms nor disconfirms."
        
        elif prediction.diagnostic_type == DiagnosticType.DOUBLY_DECISIVE:
            # Both necessary and sufficient
            if has_support and not has_contradiction:
                return TestResult.PASS, f"DOUBLY DECISIVE PASSED: Hypothesis confirmed and alternatives eliminated. Supporting: {len(supporting)}"
            else:
                return TestResult.FAIL, f"DOUBLY DECISIVE FAILED: Hypothesis eliminated. Supporting: {len(supporting)}, Contradicting: {len(contradicting)}"
        
        elif prediction.diagnostic_type == DiagnosticType.STRAW_IN_WIND:
            # Neither necessary nor sufficient - provides weak evidence
            if has_support:
                return TestResult.PASS, f"STRAW IN WIND PASSED: Weak support for hypothesis. Supporting: {len(supporting)}"
            else:
                return TestResult.INCONCLUSIVE, f"STRAW IN WIND INCONCLUSIVE: No clear support found."
        
        return TestResult.INCONCLUSIVE, "Unable to determine test result"
    
    def _calculate_confidence(self, supporting: List[str], contradicting: List[str], 
                            prediction: TestPrediction) -> float:
        """Calculate confidence level for test result"""
        total_evidence = len(supporting) + len(contradicting)
        if total_evidence == 0:
            return 0.3  # Low confidence with no evidence
        
        support_ratio = len(supporting) / total_evidence
        evidence_volume_bonus = min(total_evidence * 0.1, 0.3)  # Bonus for more evidence
        
        base_confidence = support_ratio * 0.7 + evidence_volume_bonus
        return min(base_confidence, 0.95)  # Cap at 95% confidence
    
    def _assess_elimination_implications(self, prediction: TestPrediction, 
                                       result: TestResult) -> List[str]:
        """Determine which hypotheses are eliminated by test result"""
        implications = []
        
        if result == TestResult.FAIL:
            if prediction.diagnostic_type in [DiagnosticType.HOOP, DiagnosticType.DOUBLY_DECISIVE]:
                implications.append(f"Hypothesis {prediction.hypothesis_id} ELIMINATED")
            elif prediction.diagnostic_type == DiagnosticType.SMOKING_GUN:
                implications.append(f"Hypothesis {prediction.hypothesis_id} NOT CONFIRMED")
        
        elif result == TestResult.PASS:
            if prediction.diagnostic_type in [DiagnosticType.SMOKING_GUN, DiagnosticType.DOUBLY_DECISIVE]:
                implications.append(f"Hypothesis {prediction.hypothesis_id} STRONGLY SUPPORTED")
        
        return implications
    
    def systematic_hypothesis_evaluation(self) -> Dict[str, HypothesisAssessment]:
        """
        Perform systematic Van Evera evaluation of all hypotheses.
        Returns complete academic assessment with Bayesian updating.
        """
        print("[VAN_EVERA_TESTING] Starting systematic hypothesis evaluation...")
        
        assessments = {}
        
        for hypothesis in self.hypotheses:
            hypothesis_id = hypothesis['id']
            print(f"[VAN_EVERA_TESTING] Testing hypothesis: {hypothesis_id}")
            
            # Generate testable predictions
            predictions = self.generate_testable_predictions(hypothesis)
            print(f"[VAN_EVERA_TESTING] Generated {len(predictions)} predictions for {hypothesis_id}")
            
            # Test each prediction
            test_results = []
            for prediction in predictions:
                result = self.evaluate_prediction(prediction)
                test_results.append(result)
                print(f"[VAN_EVERA_TESTING] {prediction.diagnostic_type.value} test: {result.test_result.value}")
            
            # Calculate overall assessment with Bayesian updating
            prior_prob = 0.5  # Neutral prior
            posterior_prob = self._calculate_posterior_probability(prior_prob, test_results)
            
            # Determine overall status
            overall_status = self._determine_overall_status(test_results, posterior_prob)
            
            # Generate academic conclusion
            academic_conclusion = self._generate_academic_conclusion(hypothesis, test_results, posterior_prob)
            
            # Calculate confidence interval
            confidence_interval = self._calculate_confidence_interval(posterior_prob, test_results)
            
            assessment = HypothesisAssessment(
                hypothesis_id=hypothesis_id,
                description=hypothesis.get('properties', {}).get('description', ''),
                prior_probability=prior_prob,
                posterior_probability=posterior_prob,
                test_results=test_results,
                overall_status=overall_status,
                confidence_interval=confidence_interval,
                academic_conclusion=academic_conclusion
            )
            
            assessments[hypothesis_id] = assessment
        
        print(f"[VAN_EVERA_TESTING] Completed evaluation of {len(assessments)} hypotheses")
        return assessments
    
    def _calculate_posterior_probability(self, prior: float, test_results: List[TestEvaluation]) -> float:
        """Calculate posterior probability using Bayesian updating"""
        log_odds = math.log(prior / (1 - prior))  # Convert to log odds
        
        for result in test_results:
            if result.test_result == TestResult.PASS:
                # Positive evidence - increase log odds
                log_odds += result.confidence_level * 2
            elif result.test_result == TestResult.FAIL:
                # Negative evidence - decrease log odds  
                log_odds -= result.confidence_level * 3
            # INCONCLUSIVE results don't change log odds
        
        # Convert back to probability
        odds = math.exp(log_odds)
        return odds / (1 + odds)
    
    def _determine_overall_status(self, test_results: List[TestEvaluation], posterior: float) -> str:
        """Determine overall hypothesis status"""
        failed_decisive_tests = sum(1 for r in test_results 
                                  if r.test_result == TestResult.FAIL and 
                                  any('ELIMINATED' in imp for imp in r.elimination_implications))
        
        if failed_decisive_tests > 0:
            return "ELIMINATED"
        elif posterior > 0.8:
            return "STRONGLY_SUPPORTED" 
        elif posterior > 0.6:
            return "SUPPORTED"
        elif posterior > 0.4:
            return "INCONCLUSIVE"
        else:
            return "WEAKENED"
    
    def _generate_academic_conclusion(self, hypothesis: Dict, test_results: List[TestEvaluation], 
                                    posterior: float) -> str:
        """Generate academic-quality conclusion"""
        hyp_desc = hypothesis.get('properties', {}).get('description', 'Unknown hypothesis')
        
        # Count test results by type
        hoop_results = [r for r in test_results if 'HOOP' in r.reasoning]
        smoking_gun_results = [r for r in test_results if 'SMOKING GUN' in r.reasoning]
        decisive_results = [r for r in test_results if 'DOUBLY DECISIVE' in r.reasoning]
        
        conclusion = f"HYPOTHESIS: {hyp_desc}\n\n"
        
        # Hoop test analysis
        if hoop_results:
            hoop_passes = sum(1 for r in hoop_results if r.test_result == TestResult.PASS)
            conclusion += f"NECESSARY CONDITION ANALYSIS: {hoop_passes}/{len(hoop_results)} hoop tests passed. "
            if hoop_passes == len(hoop_results):
                conclusion += "Hypothesis remains viable - all necessary conditions satisfied.\n\n"
            else:
                conclusion += "Hypothesis ELIMINATED - failed necessary condition test.\n\n"
                return conclusion
        
        # Smoking gun analysis
        if smoking_gun_results:
            smoking_gun_passes = sum(1 for r in smoking_gun_results if r.test_result == TestResult.PASS)
            conclusion += f"SUFFICIENT CONDITION ANALYSIS: {smoking_gun_passes}/{len(smoking_gun_results)} smoking gun tests passed. "
            if smoking_gun_passes > 0:
                conclusion += "Strong confirming evidence found.\n\n"
            else:
                conclusion += "No decisive confirming evidence.\n\n"
        
        # Overall assessment
        conclusion += f"OVERALL ASSESSMENT: Posterior probability = {posterior:.2f}. "
        if posterior > 0.8:
            conclusion += "Hypothesis STRONGLY SUPPORTED by systematic testing."
        elif posterior > 0.6:
            conclusion += "Hypothesis SUPPORTED with moderate confidence."
        elif posterior > 0.4:
            conclusion += "Evidence INCONCLUSIVE - requires additional testing."
        else:
            conclusion += "Hypothesis WEAKENED by available evidence."
        
        return conclusion
    
    def _calculate_confidence_interval(self, posterior: float, test_results: List[TestEvaluation]) -> Tuple[float, float]:
        """Calculate confidence interval for posterior probability"""
        n_tests = len(test_results)
        if n_tests == 0:
            return (max(0, posterior - 0.3), min(1, posterior + 0.3))
        
        # Confidence narrows with more tests
        margin = 0.4 / math.sqrt(n_tests)
        return (max(0, posterior - margin), min(1, posterior + margin))

# Integration function
def perform_van_evera_testing(graph_data: Dict) -> Dict:
    """
    Main entry point for systematic Van Evera hypothesis testing.
    Returns comprehensive academic assessment results.
    """
    engine = VanEveraTestingEngine(graph_data)
    return engine.systematic_hypothesis_evaluation()
```

**Integration Point**: `core/analyze.py`
**Add after alternative hypothesis generation**:
```python
# Import at top  
from core.van_evera_testing_engine import perform_van_evera_testing

# Add in format_html_analysis function
if 'graph_data' in results:
    print("[ANALYSIS] Performing systematic Van Evera hypothesis testing...")
    van_evera_results = perform_van_evera_testing(results['graph_data'])
    results['van_evera_assessment'] = van_evera_results
```

**HTML Integration**: `core/streaming_html.py`
**Add new section method**:
```python
def _write_van_evera_section(self, van_evera_results: Dict):
    """Write systematic Van Evera hypothesis testing results"""
    self.writer.write_section_start("van-evera", "Van Evera Systematic Analysis", 
                                   "Academic process tracing with diagnostic tests")
    
    for hyp_id, assessment in van_evera_results.items():
        assessment_html = f"""
            <div class="hypothesis-assessment mb-4">
                <h6>{assessment.description}</h6>
                <div class="assessment-summary">
                    <p><strong>Status:</strong> {assessment.overall_status}</p>
                    <p><strong>Posterior Probability:</strong> {assessment.posterior_probability:.2f} 
                       (CI: {assessment.confidence_interval[0]:.2f}-{assessment.confidence_interval[1]:.2f})</p>
                </div>
                <div class="test-results">
                    <h7>Diagnostic Test Results:</h7>
        """
        
        for test in assessment.test_results:
            test_html = f"""
                    <div class="test-result {test.test_result.value.lower()}">
                        <strong>{test.test_result.value}:</strong> {test.reasoning}
                        <br><small>Confidence: {test.confidence_level:.2f}</small>
                    </div>
            """
            assessment_html += test_html
        
        assessment_html += f"""
                </div>
                <div class="academic-conclusion">
                    <h7>Academic Conclusion:</h7>
                    <pre>{assessment.academic_conclusion}</pre>
                </div>
            </div>
        """
        
        self.writer.write_section_content(assessment_html)
    
    self.writer.write_section_end()
```

**Add to analysis sections in streaming HTML**:
```python
# In _write_analysis_sections method, add:
if results.get('van_evera_assessment'):
    self._write_van_evera_section(results['van_evera_assessment'])
```

**Validation**:
```bash
python process_trace_advanced.py --project revolutions --analyze-only --graph-file "output_data/revolutions/revolutions_20250805_122000_graph.json"
grep -c "HOOP TEST\|SMOKING GUN\|DOUBLY DECISIVE" output_data/revolutions/revolutions_*_analysis_*.html
# Expected: Multiple systematic test results with pass/fail determinations
```

## ðŸŽ¯ IMPLEMENTATION TIMELINE

**WEEK 1-2: Foundation (Diagnostic Rebalancing)**
- Implement diagnostic rebalancing system
- Test and validate evidence distribution
- Integrate with existing analysis pipeline

**WEEK 2-3: Competition (Alternative Hypotheses)** 
- Generate 8 competing explanations
- Create systematic competitive relationships
- Validate theoretical coverage

**WEEK 3-5: Testing Engine (Van Evera Logic)**
- Implement systematic hypothesis testing
- Add Bayesian probability updating
- Generate academic conclusions

**WEEK 5-6: Polish & Integration**
- Refine HTML output for academic presentation
- Add confidence intervals and uncertainty quantification  
- Final testing and validation

**WEEK 7-8: Academic Quality Verification**
- Compare output to published Van Evera analyses
- Refine academic prose generation
- Final academic quality assessment

## ðŸ”§ VALIDATION STRATEGY

**After each priority, validate**:
```bash
# Test complete analysis pipeline
python process_trace_advanced.py --project revolutions --analyze-only --graph-file "output_data/revolutions/revolutions_20250805_122000_graph.json"

# Check diagnostic distribution
python -c "
import json
data = json.load(open('output_data/revolutions/revolutions_*_graph.json'))  
edges = [e for e in data['edges'] if e.get('properties', {}).get('diagnostic_type')]
types = [e['properties']['diagnostic_type'] for e in edges]
from collections import Counter
print('Diagnostic distribution:', Counter(types))
"

# Verify academic output quality
grep -c "HOOP TEST.*PASS\|SMOKING GUN.*PASS\|ELIMINATED\|STRONGLY SUPPORTED" output_data/revolutions/revolutions_*_analysis_*.html
# Expected: Multiple systematic test results with clear academic conclusions
```

## ðŸŽ¯ SUCCESS CRITERIA

**Current Academic Quality: 37%**
**Target After Implementation: 85%**

**Before Implementation**:
- Diagnostic Tests: 67% smoking gun (confirmation bias)
- Alternative Hypotheses: 2 weak alternatives
- Testing Logic: Descriptive evidence display
- Academic Conclusion: Technical report format

**After Implementation**:
- Diagnostic Tests: 25% hoop, 25% smoking gun, 15% doubly decisive, 35% straw-in-wind  
- Alternative Hypotheses: 8 robust competing explanations
- Testing Logic: Systematic prediction â†’ test â†’ elimination sequence
- Academic Conclusion: Scholarly analysis with Bayesian updating

## ðŸ”§ DEVELOPMENT ENVIRONMENT

### **Verified Working Setup**
- **Python 3.8+** with existing dependencies
- **API**: `gemini-2.5-flash` via `.env` file (confirmed working)
- **Test Data**: `output_data/revolutions/revolutions_20250805_122000_graph.json` (160 nodes, 203 edges)

### **New Dependencies**
```bash
# Add to requirements.txt if needed
dataclasses  # For structured test results
enum34      # For test result enums
```

**Repository is clean, all systems functional. Focus on implementing systematic Van Evera methodology for publication-quality academic analysis.**